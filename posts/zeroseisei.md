### ゼロから作るDeep Learning 5 生成モデル編

---

<p>
この本は社会人になり，初めて真面目に読んだ機械学習の1冊目の本です．
友人と約5ヶ月ほどかけて読了しました．
各章ごとに簡単に感想を述べていこうと思います．
</p>

#### 1章　正規分布

<p>
この章では，確率論の基本的なことが書かれています．
学部生の講義を受けたことがあればさらっと流して良いと思います．
具体的には，確率分布や確率密度関数などの定義がわかっており，中心極限定理などがわかっていれば読み飛ばしても良いかと思います．
</p>
<p>
今回は確率論に詳しい友人とセミナー形式で読んだこともあり，せっかくなのでということで測度論的確率論の基礎を押さえました．
</p>
<p>
この章の最後には，身の回りで正規分布が現れる例が書かれてます．
</p>

#### 2章　最尤推定

<p>
この章では，題名にある生成モデルの話から始まります．
この章も学部生の確率論や統計の講義を受けたことがあればさらっと流して良いと思います．
具体的には，母集団や最尤推定などがわかっていれば読み飛ばしても良いかと思います．
</p>
<p>
SOCR Data(1993年の香港における18歳の身長調査からできた架空のデータ)を使って実際にヒストグラムを描写するコードも学べたりします．
matplotlibなどに慣れていればさらっと読めるかと思います．
</p>
<p>
この章の最後には，深層学習についてもほんの少し書かれています．
</p>

#### 3章　多次元正規分布

<p>
この章では，ここまで書いてあったことを多次元化する話が書いてあります．
この章も学部生の確率論や線形代数の講義を受けたことがあればさらっと流して良いと思います．
具体的には，1,2章がわかっていて線形代数がわかっていればさらっと読めると思います．
</p>

#### 4章　混合ガウスモデル

<p>
この章では，混合ガウスモデルについて書いてあります．
</p>
<p>
1章で身の回りに現れる正規分布の話がありましたが，世の中そんなに甘くなく，正規分布で表せない現象もたくさんあります．
混合ガウスモデルは複数の正規分布を組み合わせるアプローチです．
実際に男女混合の身長の分布など具体例を見ることから始まります．
また，さらっと確率論の復習がされます．
条件付き確率や乗法定理がさらっと書かれていますが，この本ではこの辺はよく使うので重要に感じます．
</p>
<p>
この章の最後には，混合ガウスモデルのパラメータ推定が容易ではなく，解析的には解けないことを見ます．
</p>

#### 5章　EMアルゴリズム

<p>
この章では，EMアルゴリズムについて書いてあります．
前章の最後で混合ガウスモデルのパラメータ推定が容易ではなく，解析的には解けないことを見ました．
これを解決する方法の1つがEMアルゴリズムです．
</p>
<p>
まずは，2つの確率分布の測る尺度としてKLダイバージェンスが導入されます．
幾何をやっている人間としてはこの概念自体とても気になります．
KLダイバージェンスを考えることで対数尤度がELBO(Evidence Lower BOund)とKLダイバージェンスの和で表されることを見ます．
</p>
<p>
KLダイバージェンスやモンテカルロ法を用いて最尤推定を行い，混合ガウスモデルに対して，EMアルゴリズムを適用します．
EMアルゴリズムはパラメータを固定して確率分布を更新してELBOを対数尤度に近づけるEステップとパラメータを更新してELBOをsum-logの形にすることで解析的に求めるMステップに分けられます．
結構計算が必要ですが，落ち着いて読めば難しいものではないと思います．
</p>
<p>
この章の最後には，EMアルゴリズムの実装について書いてあります．
</p>

#### 6章　ニューラルネットワーク

<p>
この章では，ニューラルネットワークについて書いてあります．
基本的なニューラルネットワークのことを知っていたり，Pytorchを使ったことがあればさらっと流して良いと思います．
</p>
<p>
コードベースに勾配法や線形回帰やニューラルネットワークについて書いてあります．
最後にはMNISTデータセットについても少し書いてあります．
</p>

#### 7章　変分オートエンコーダ(VAE)
<p>
この章では，変分オートエンコーダについて書いてあります．
4章で混合ガウスモデルを見たが，ニューラルネットワークを用いることでより複雑なモデルを考えることができる．
その一例が変分オートエンコーダである．
変分オートエンコーダでは，学習データに合わせて殉難に形が決まる確率分布を表現できる．
</p>
<p>
変分オートエンコーダでは，潜在変数を固定の正規分布から生成し，ニューラルネットワークによって，観測変数へ変換します．
このニューラルネットワークは潜在変数から観測変数に変換するため，デコーダともよばれます．
</p>
<p>
理論的にはEMアルゴリズムを用いて変分オートエンコーダの学習が可能だが，実際には計算量の制約により，実現が困難であることを計算により見ます．
</p>
<p>
この章の最後には，変分オートエンコードの実装の話も結構しっかり書いてあります．
</p>

#### 8章　拡散モデルの理論

<p>
この章では，いよいよ拡散モデルの話が始まります．
7章では，変分オートエンコーダを見ました．
その潜在変数を階層化して，階層型変分オートエンコーダを学び，さらに発展させて，拡散モデルを学びます．
正確には，ここではDDPM(Deenosing Diffusion Probabilistic Models)を扱います．
</p>
<p>
階層型変分オートエンコーダではマルコフ性を仮定することでパラメータの増加を防ぎます．
詳しくは付録Cに書いてあります．
</p>
<p>
階層型変分オートエンコーダに対し，以下2点を変更することで拡散モデルへと発展させます．
</p>

- 観測変数と潜在変数の次元数を同じにする．

- エンコーダは固定の正規分布によるノイズを参加する．

<p>
拡散モデルにはノイズを追加する拡散過程とノイズを除去する逆拡散過程という2つの処理があります．
</p>
<p>
拡散モデルでも変分オートエンコーダと同様にELBOの最適化をします．
ここでは，サンプリングがT回，2回，1回という様に段々サンプリングの回数を減らしていきます．
適度な関数解析と確率論の知識があればさらっと読めるかなと思います．
一様分布やガウスノイズの性質やベイズの定理をうまく使うことでサンプリングの回数を減らすことができます．
</p>

<p>
この章の最後には，今まで学んだことを疑似コードで見ることができます．
</p>

#### 9章　拡散モデルの実装

<p>
この章では，拡散モデルの実装について書いてあります．
8章では拡散モデルの理論について学べたのでこの章では実装について学んでいきます．
拡散モデルでよく使われるU-Netというニューラルネットワークについて学び，時刻データを効率よくシャリするための正弦波位置エンコーディングについて学びます．
最後にはMNISTのデータセットを用いて拡散モデルの学習を行います．
</p>
<p>
コードが豊富に書いてあるので実際に実装しながら学ぶことができます．
</p>

#### 10章　拡散モデルの応用

#### 最後に

---

**[機械学習の文献に戻る](/posts/20190506)**